# Assignment 1: Predict Weight from Height (Univariate Linear Regression)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 1. Load and describe dataset
df = pd.read_csv("height_weight.csv")  # columns: 'Height', 'Weight'

print("First rows:\n", df.head())
print("\nShape:", df.shape)
print("\nDescription:\n", df.describe())
print("\nMissing values:\n", df.isna().sum())

# 2. Outlier detection using IQR on Weight (you can do same for Height)
Q1 = df["Weight"].quantile(0.25)
Q3 = df["Weight"].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR

# Keep only non-outliers (optional; comment this block to keep all)
df = df[(df["Weight"] >= lower) & (df["Weight"] <= upper)]

# 3. Feature selection & preprocessing
X = df[["Height"]]        # feature
y = df["Weight"]          # target

# For simple linear regression, scaling is not required for correctness,
# but you could standardize if needed. Here we skip scaling.

# 4. Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 5. Train model
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

# 6. Model equation
coef = lin_reg.coef_[0]
intercept = lin_reg.intercept_

print(f"\nModel: Weight = {intercept:.3f} + {coef:.3f} * Height")

# Interpretation example (you can write in your report):
# For every 1 unit increase in Height, Weight increases by approximately `coef` units
# on average, keeping everything else constant (here only one feature).

# 7. Metrics: MSE & R^2
y_train_pred = lin_reg.predict(X_train)
y_test_pred = lin_reg.predict(X_test)

train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"\nTrain MSE: {train_mse:.3f}, Train R^2: {train_r2:.3f}")
print(f"Test  MSE: {test_mse:.3f}, Test  R^2: {test_r2:.3f}")

# Comment in report: 
# - If train and test R^2 are both high and similar → good generalization.
# - If train R^2 >> test R^2 → overfitting.
# - If both low → underfitting.

# 8. Scatter plot + regression line
plt.figure()
plt.scatter(X, y, label="Data")
# line over sorted X for nicer plot
x_line = np.linspace(X["Height"].min(), X["Height"].max(), 100).reshape(-1, 1)
y_line = lin_reg.predict(x_line)
plt.plot(x_line, y_line, label="Fitted line")
plt.xlabel("Height")
plt.ylabel("Weight")
plt.title("Height vs Weight with Regression Line")
plt.legend()
plt.show()

# 9. Residual plot
residuals = y_test - y_test_pred
plt.figure()
plt.scatter(y_test_pred, residuals)
plt.axhline(0, linestyle="--")
plt.xlabel("Predicted Weight")
plt.ylabel("Residuals")
plt.title("Residual Plot")
plt.show()

# In report: if residuals are randomly scattered around 0 → assumptions okay.
